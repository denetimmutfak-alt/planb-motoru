#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ULTRA RISK MODULE - ENHANCED
Arkada≈ü fikirlerinin uygulanmasƒ± - Value at Risk, Monte Carlo, Stress Testing
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
import logging
from dataclasses import dataclass
import warnings
warnings.filterwarnings('ignore')

from multi_expert_engine import ExpertModule, ModuleResult

logger = logging.getLogger(__name__)

@dataclass
class RiskEvent:
    """Risk olay tanƒ±mƒ±"""
    risk_type: str  # "market", "credit", "liquidity", "operational"
    severity: float  # 0-1 arasƒ± risk ≈üiddeti
    probability: float  # 0-1 arasƒ± ger√ßekle≈üme olasƒ±lƒ±ƒüƒ±
    impact: float  # Finansal etki ($)
    time_horizon: int  # G√ºn cinsinden zaman ufku
    mitigation_strategies: List[str]

class UltraRiskModule(ExpertModule):
    """
    Ultra Risk Management Module
    Arkada≈ü √∂nerisi: VaR, Monte Carlo, Stress Testing integrated approach
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        super().__init__("Ultra Risk Management", config)
        
        self.description = "Advanced VaR, Monte Carlo simulation, and stress testing"
        self.version = "2.0.0"  # Enhanced version
        self.dependencies = ["numpy", "pandas", "scipy"]
        
        # Risk parameters
        self.confidence_levels = [0.95, 0.99, 0.999]  # VaR confidence levels
        self.time_horizons = [1, 5, 10, 22]  # Days
        self.monte_carlo_simulations = 10000
        
        # Risk models configuration
        self.risk_models = {
            "parametric_var": {"enabled": True, "weight": 0.3},
            "historical_var": {"enabled": True, "weight": 0.3},
            "monte_carlo_var": {"enabled": True, "weight": 0.4}
        }
        
        # Stress test scenarios
        self.stress_scenarios = self._initialize_stress_scenarios()
        
        # Risk thresholds
        self.risk_limits = {
            "max_daily_var": 0.05,  # %5 g√ºnl√ºk VaR limiti
            "max_drawdown": 0.20,   # %20 maksimum d√º≈ü√º≈ü
            "min_sharpe": 1.0,      # Minimum Sharpe ratio
            "max_correlation": 0.8   # Maksimum korelasyon
        }
        
        logger.info("Ultra Risk Management Module initialized")
    
    def _initialize_stress_scenarios(self) -> Dict[str, Dict]:
        """Stress test senaryolarƒ±nƒ± ba≈ülat"""
        return {
            "market_crash": {
                "description": "2008 benzeri market √ß√∂k√º≈ü√º",
                "market_shock": -0.30,  # %30 d√º≈ü√º≈ü
                "volatility_multiplier": 3.0,
                "correlation_breakdown": 0.9,  # Korelasyonlar 0.9'a √ßƒ±kar
                "probability": 0.05
            },
            "liquidity_crisis": {
                "description": "Likidite krizi",
                "market_shock": -0.15,
                "volatility_multiplier": 2.0,
                "bid_ask_widening": 5.0,  # 5x bid-ask spread
                "probability": 0.10
            },
            "currency_crisis": {
                "description": "Para birimi krizi",
                "fx_shock": -0.25,  # %25 deval√ºasyon
                "inflation_shock": 0.15,  # %15 enflasyon
                "interest_rate_shock": 0.05,  # %5 faiz artƒ±≈üƒ±
                "probability": 0.08
            },
            "pandemic_scenario": {
                "description": "Pandemi benzeri saƒülƒ±k krizi",
                "market_shock": -0.35,
                "volatility_multiplier": 4.0,
                "recovery_time": 180,  # 6 ay recovery
                "probability": 0.03
            },
            "geopolitical_crisis": {
                "description": "Jeopolitik kriz",
                "market_shock": -0.20,
                "commodity_shock": 0.50,  # %50 emtia fiyat artƒ±≈üƒ±
                "safe_haven_premium": 0.30,
                "probability": 0.12
            }
        }
    
    def get_required_fields(self) -> List[str]:
        """Gerekli veri alanlarƒ±"""
        return ["symbol", "close", "volume", "timestamp"]
    
    def calculate_parametric_var(self, returns: np.ndarray, confidence_level: float = 0.95, 
                                time_horizon: int = 1) -> Dict[str, float]:
        """Parametrik VaR hesaplama"""
        try:
            # Return istatistikleri
            mean_return = np.mean(returns)
            std_return = np.std(returns)
            
            # Normal daƒüƒ±lƒ±m varsayƒ±mƒ± ile VaR
            from scipy.stats import norm
            z_score = norm.ppf(1 - confidence_level)
            
            # Zaman ufku ayarlamasƒ±
            horizon_mean = mean_return * time_horizon
            horizon_std = std_return * np.sqrt(time_horizon)
            
            var = -(horizon_mean + z_score * horizon_std)
            
            # Expected Shortfall (CVaR)
            expected_shortfall = -(horizon_mean + horizon_std * norm.pdf(z_score) / (1 - confidence_level))
            
            return {
                "var": var,
                "expected_shortfall": expected_shortfall,
                "confidence": confidence_level,
                "time_horizon": time_horizon,
                "model_type": "parametric"
            }
            
        except Exception as e:
            logger.error(f"Parametric VaR calculation error: {str(e)}")
            return {"var": 0.05, "expected_shortfall": 0.07, "confidence": 0.1, "time_horizon": time_horizon, "model_type": "parametric"}
    
    def calculate_historical_var(self, returns: np.ndarray, confidence_level: float = 0.95,
                                time_horizon: int = 1) -> Dict[str, float]:
        """Tarihsel VaR hesaplama"""
        try:
            # Zaman ufku i√ßin return'leri scale et
            if time_horizon > 1:
                scaled_returns = returns * np.sqrt(time_horizon)
            else:
                scaled_returns = returns
            
            # Percentile ile VaR
            var = -np.percentile(scaled_returns, (1 - confidence_level) * 100)
            
            # Expected Shortfall
            tail_returns = scaled_returns[scaled_returns <= -var]
            if len(tail_returns) > 0:
                expected_shortfall = -np.mean(tail_returns)
            else:
                expected_shortfall = var * 1.3
            
            return {
                "var": var,
                "expected_shortfall": expected_shortfall,
                "confidence": confidence_level,
                "time_horizon": time_horizon,
                "model_type": "historical"
            }
            
        except Exception as e:
            logger.error(f"Historical VaR calculation error: {str(e)}")
            return {"var": 0.05, "expected_shortfall": 0.07, "confidence": 0.1, "time_horizon": time_horizon, "model_type": "historical"}
    
    def calculate_monte_carlo_var(self, returns: np.ndarray, confidence_level: float = 0.95,
                                 time_horizon: int = 1, simulations: int = 10000) -> Dict[str, float]:
        """Monte Carlo VaR hesaplama"""
        try:
            mean_return = np.mean(returns)
            std_return = np.std(returns)
            
            # Monte Carlo sim√ºlasyonu
            np.random.seed(42)  # Reproducible results
            simulated_returns = np.random.normal(
                mean_return * time_horizon,
                std_return * np.sqrt(time_horizon),
                simulations
            )
            
            # VaR hesaplama
            var = -np.percentile(simulated_returns, (1 - confidence_level) * 100)
            
            # Expected Shortfall
            tail_returns = simulated_returns[simulated_returns <= -var]
            if len(tail_returns) > 0:
                expected_shortfall = -np.mean(tail_returns)
            else:
                expected_shortfall = var * 1.3
            
            return {
                "var": var,
                "expected_shortfall": expected_shortfall,
                "confidence": confidence_level,
                "time_horizon": time_horizon,
                "model_type": "monte_carlo",
                "simulations": simulations
            }
            
        except Exception as e:
            logger.error(f"Monte Carlo VaR calculation error: {str(e)}")
            return {"var": 0.05, "expected_shortfall": 0.07, "confidence": 0.1, "time_horizon": time_horizon, "model_type": "monte_carlo"}
    
    def calculate_risk_metrics(self, returns: np.ndarray, benchmark_returns: np.ndarray = None) -> Dict[str, float]:
        """Kapsamlƒ± risk metrikleri"""
        try:
            # Temel metrikler
            annual_return = np.mean(returns) * 252
            annual_volatility = np.std(returns) * np.sqrt(252)
            
            # Sharpe Ratio (risk-free rate = 0 varsayƒ±mƒ±)
            sharpe_ratio = annual_return / annual_volatility if annual_volatility > 0 else 0
            
            # Sortino Ratio
            downside_returns = returns[returns < 0]
            downside_deviation = np.std(downside_returns) * np.sqrt(252) if len(downside_returns) > 0 else 0.01
            sortino_ratio = annual_return / downside_deviation if downside_deviation > 0 else 0
            
            # Maximum Drawdown
            cumulative_returns = np.cumprod(1 + returns)
            rolling_max = np.maximum.accumulate(cumulative_returns)
            drawdowns = (cumulative_returns - rolling_max) / rolling_max
            max_drawdown = np.min(drawdowns)
            
            # Calmar Ratio
            calmar_ratio = annual_return / abs(max_drawdown) if max_drawdown != 0 else 0
            
            # VaR ve CVaR (g√ºnl√ºk)
            var_95 = -np.percentile(returns, 5)
            tail_returns = returns[returns <= -var_95]
            cvar_95 = -np.mean(tail_returns) if len(tail_returns) > 0 else var_95 * 1.2
            
            metrics = {
                "annual_return": annual_return,
                "annual_volatility": annual_volatility,
                "sharpe_ratio": sharpe_ratio,
                "sortino_ratio": sortino_ratio,
                "max_drawdown": max_drawdown,
                "calmar_ratio": calmar_ratio,
                "var_95": var_95,
                "cvar_95": cvar_95,
                "downside_deviation": downside_deviation
            }
            
            # Benchmark ile kar≈üƒ±la≈ütƒ±rma
            if benchmark_returns is not None and len(benchmark_returns) == len(returns):
                excess_returns = returns - benchmark_returns
                tracking_error = np.std(excess_returns) * np.sqrt(252)
                information_ratio = np.mean(excess_returns) * 252 / tracking_error if tracking_error > 0 else 0
                
                # Beta hesaplama
                covariance = np.cov(returns, benchmark_returns)[0, 1]
                benchmark_variance = np.var(benchmark_returns)
                beta = covariance / benchmark_variance if benchmark_variance > 0 else 1.0
                
                # Alpha hesaplama
                alpha = annual_return - beta * np.mean(benchmark_returns) * 252
                
                metrics.update({
                    "tracking_error": tracking_error,
                    "information_ratio": information_ratio,
                    "beta": beta,
                    "alpha": alpha
                })
            
            return metrics
            
        except Exception as e:
            logger.error(f"Risk metrics calculation error: {str(e)}")
            return {
                "annual_return": 0.0,
                "annual_volatility": 0.2,
                "sharpe_ratio": 0.0,
                "max_drawdown": -0.1,
                "var_95": 0.03,
                "cvar_95": 0.05
            }
    
    def run_stress_tests(self, current_price: float, returns: np.ndarray) -> Dict[str, Dict]:
        """Stress testleri √ßalƒ±≈ütƒ±r"""
        stress_results = {}
        
        for scenario_name, scenario in self.stress_scenarios.items():
            try:
                # Temel market shock
                stressed_price = current_price * (1 + scenario.get("market_shock", 0))
                
                # Volatility impact
                vol_multiplier = scenario.get("volatility_multiplier", 1.0)
                stressed_volatility = np.std(returns) * vol_multiplier
                
                # Expected loss calculation
                price_loss = (stressed_price - current_price) / current_price
                volatility_impact = (stressed_volatility - np.std(returns)) * 2  # 2-sigma impact
                
                total_expected_loss = price_loss + volatility_impact
                
                # Risk-adjusted probability
                probability = scenario.get("probability", 0.05)
                risk_adjusted_impact = total_expected_loss * probability
                
                stress_results[scenario_name] = {
                    "scenario_description": scenario.get("description", ""),
                    "stressed_price": stressed_price,
                    "price_impact": price_loss,
                    "volatility_impact": volatility_impact,
                    "total_expected_loss": total_expected_loss,
                    "probability": probability,
                    "risk_adjusted_impact": risk_adjusted_impact,
                    "severity_score": abs(total_expected_loss) * 100  # 0-100 scale
                }
                
            except Exception as e:
                logger.error(f"Stress test error for {scenario_name}: {str(e)}")
                stress_results[scenario_name] = {
                    "scenario_description": scenario.get("description", ""),
                    "total_expected_loss": -0.05,
                    "probability": 0.05,
                    "severity_score": 5.0
                }
        
        return stress_results
    
    def simulate_price_data(self, symbol: str, periods: int = 252) -> pd.DataFrame:
        """Fiyat verisi sim√ºlasyonu"""
        np.random.seed(42)
        dates = pd.date_range(start=datetime.now() - timedelta(days=periods), periods=periods, freq='D')
        
        # Base parameters
        initial_price = 100.0
        annual_return = 0.08
        annual_volatility = 0.25
        
        # Daily parameters
        daily_return = annual_return / 252
        daily_vol = annual_volatility / np.sqrt(252)
        
        # Generate price path
        returns = np.random.normal(daily_return, daily_vol, periods)
        prices = [initial_price]
        
        for ret in returns[1:]:
            new_price = prices[-1] * (1 + ret)
            prices.append(max(1.0, new_price))  # Fiyat 1'den k√º√ß√ºk olamaz
        
        volumes = np.random.randint(100000, 1000000, periods)
        
        df = pd.DataFrame({
            "timestamp": dates,
            "close": prices,
            "volume": volumes,
            "returns": [0] + list(np.diff(np.log(prices)))
        })
        
        return df
    
    def prepare_features(self, raw_data: Dict[str, Any]) -> pd.DataFrame:
        """Risk analizi i√ßin veri hazƒ±rlama"""
        try:
            symbol = raw_data["symbol"]
            
            # Simulated price data
            price_data = self.simulate_price_data(symbol)
            
            # Returns calculation
            returns = price_data["returns"].values[1:]  # ƒ∞lk 0'ƒ± √ßƒ±kar
            current_price = price_data["close"].iloc[-1]
            
            # VaR calculations (farklƒ± modeller)
            var_95_parametric = self.calculate_parametric_var(returns, 0.95, 1)
            var_95_historical = self.calculate_historical_var(returns, 0.95, 1)
            var_95_monte_carlo = self.calculate_monte_carlo_var(returns, 0.95, 1)
            
            # Risk metrics
            risk_metrics = self.calculate_risk_metrics(returns)
            
            # Stress tests
            stress_results = self.run_stress_tests(current_price, returns)
            
            # Aggregate stress impact
            worst_case_loss = max([result["total_expected_loss"] for result in stress_results.values()])
            avg_stress_impact = np.mean([result["risk_adjusted_impact"] for result in stress_results.values()])
            
            # Features dictionary
            features_dict = {
                "symbol": symbol,
                "current_price": current_price,
                "returns_mean": np.mean(returns),
                "returns_std": np.std(returns),
                
                # VaR metrics
                "var_95_parametric": var_95_parametric["var"],
                "var_95_historical": var_95_historical["var"],
                "var_95_monte_carlo": var_95_monte_carlo["var"],
                "cvar_95_parametric": var_95_parametric["expected_shortfall"],
                "cvar_95_historical": var_95_historical["expected_shortfall"],
                "cvar_95_monte_carlo": var_95_monte_carlo["expected_shortfall"],
                
                # Risk metrics
                "sharpe_ratio": risk_metrics.get("sharpe_ratio", 0),
                "sortino_ratio": risk_metrics.get("sortino_ratio", 0),
                "max_drawdown": risk_metrics.get("max_drawdown", 0),
                "calmar_ratio": risk_metrics.get("calmar_ratio", 0),
                "annual_volatility": risk_metrics.get("annual_volatility", 0),
                
                # Stress test results
                "worst_case_loss": worst_case_loss,
                "avg_stress_impact": avg_stress_impact,
                "stress_scenarios_count": len(stress_results),
                
                # Risk scores
                "var_ensemble": np.mean([var_95_parametric["var"], var_95_historical["var"], var_95_monte_carlo["var"]]),
                "cvar_ensemble": np.mean([var_95_parametric["expected_shortfall"], var_95_historical["expected_shortfall"], var_95_monte_carlo["expected_shortfall"]])
            }
            
            # Store for inference
            self._current_var_models = {
                "parametric": var_95_parametric,
                "historical": var_95_historical,
                "monte_carlo": var_95_monte_carlo
            }
            self._current_stress_results = stress_results
            self._current_risk_metrics = risk_metrics
            
            return pd.DataFrame([features_dict])
            
        except Exception as e:
            logger.error(f"Error preparing risk features: {str(e)}")
            return pd.DataFrame([{
                "symbol": raw_data.get("symbol", "UNKNOWN"),
                "var_95_parametric": 0.05,
                "sharpe_ratio": 0.0,
                "max_drawdown": -0.1
            }])
    
    def infer(self, features: pd.DataFrame) -> ModuleResult:
        """Risk analizi √ßƒ±karƒ±mƒ±"""
        try:
            row = features.iloc[0]
            symbol = row["symbol"]
            
            # Risk score calculation (0-100, d√º≈ü√ºk risk = y√ºksek skor)
            risk_components = []
            
            # VaR component (40% weight)
            var_ensemble = row["var_ensemble"]
            var_score = max(0, 100 - (var_ensemble * 2000))  # VaR'ƒ± y√ºzdeye √ßevir ve tersine √ßevir
            risk_components.append(("var", var_score, 0.4))
            
            # Sharpe ratio component (25% weight)
            sharpe = row["sharpe_ratio"]
            sharpe_score = min(100, max(0, (sharpe + 1) * 50))  # -1 to 1 range to 0-100
            risk_components.append(("sharpe", sharpe_score, 0.25))
            
            # Drawdown component (20% weight)
            max_dd = abs(row["max_drawdown"])
            drawdown_score = max(0, 100 - (max_dd * 500))  # %20 dd = 0 puan
            risk_components.append(("drawdown", drawdown_score, 0.2))
            
            # Stress test component (15% weight)
            worst_case = abs(row["worst_case_loss"])
            stress_score = max(0, 100 - (worst_case * 200))  # %50 loss = 0 puan
            risk_components.append(("stress", stress_score, 0.15))
            
            # Weighted final score
            final_score = sum(score * weight for _, score, weight in risk_components)
            final_score = max(0, min(100, final_score))
            
            # Uncertainty calculation
            uncertainty = self._calculate_risk_uncertainty(row)
            
            # Signal types
            signal_types = []
            if final_score > 75:
                signal_types.append("low_risk")
            elif final_score > 50:
                signal_types.append("moderate_risk")
            elif final_score > 25:
                signal_types.append("high_risk")
            else:
                signal_types.append("extreme_risk")
            
            # Risk level indicators
            if var_ensemble > 0.05:
                signal_types.append("high_var")
            if abs(row["max_drawdown"]) > 0.15:
                signal_types.append("high_drawdown")
            if row["sharpe_ratio"] < 0.5:
                signal_types.append("poor_risk_adjusted_return")
            
            # Explanation
            var_models = getattr(self, '_current_var_models', {})
            stress_results = getattr(self, '_current_stress_results', {})
            
            explanation = f"Risk analizi: {final_score:.1f}/100. "
            explanation += f"VaR (95%): {var_ensemble:.1%}, "
            explanation += f"Sharpe: {row['sharpe_ratio']:.2f}, "
            explanation += f"Max DD: {row['max_drawdown']:.1%}. "
            
            if stress_results:
                worst_scenario = max(stress_results.keys(), key=lambda k: abs(stress_results[k]["total_expected_loss"]))
                explanation += f"En k√∂t√º senaryo: {worst_scenario} ({stress_results[worst_scenario]['total_expected_loss']:.1%})"
            
            # Contributing factors
            contributing_factors = {
                "var_quality": 1.0 - min(var_ensemble / 0.1, 1.0),  # Lower VaR = better
                "risk_adjusted_return": min(max(row["sharpe_ratio"] + 1, 0) / 3, 1),  # Sharpe normalized
                "drawdown_control": 1.0 - min(abs(row["max_drawdown"]) / 0.3, 1.0),
                "stress_resilience": 1.0 - min(abs(row["worst_case_loss"]) / 0.5, 1.0),
                "volatility_management": 1.0 - min(row["annual_volatility"], 1.0)
            }
            
            result = ModuleResult(
                score=final_score,
                uncertainty=uncertainty,
                type=signal_types,
                explanation=explanation,
                timestamp=datetime.now().isoformat(),
                confidence_level="",  # Auto-calculated
                contributing_factors=contributing_factors
            )
            
            logger.info(f"Risk analysis completed for {symbol}: {final_score:.2f} (uncertainty: {uncertainty:.3f})")
            return result
            
        except Exception as e:
            logger.error(f"Error in risk inference: {str(e)}")
            return self.create_fallback_result(f"Risk analysis error: {str(e)}")
    
    def _calculate_risk_uncertainty(self, features: pd.Series) -> float:
        """Risk analizi belirsizliƒüi hesapla"""
        uncertainties = []
        
        # VaR model agreement
        var_parametric = features.get("var_95_parametric", 0.05)
        var_historical = features.get("var_95_historical", 0.05)
        var_monte_carlo = features.get("var_95_monte_carlo", 0.05)
        
        var_spread = np.std([var_parametric, var_historical, var_monte_carlo])
        var_uncertainty = min(var_spread / 0.02, 1.0)  # Normalize
        uncertainties.append(var_uncertainty)
        
        # Data quality uncertainty
        returns_std = features.get("returns_std", 0.02)
        if returns_std > 0.05:  # High volatility = high uncertainty
            uncertainties.append(0.8)
        elif returns_std < 0.01:  # Very low volatility = suspicious
            uncertainties.append(0.6)
        else:
            uncertainties.append(0.3)
        
        # Stress test coverage
        stress_count = features.get("stress_scenarios_count", 0)
        if stress_count < 3:
            uncertainties.append(0.7)
        else:
            uncertainties.append(0.2)
        
        # Sharpe ratio reliability
        sharpe = features.get("sharpe_ratio", 0)
        if abs(sharpe) > 3:  # Extreme Sharpe values are suspicious
            uncertainties.append(0.8)
        else:
            uncertainties.append(0.3)
        
        return min(1.0, np.mean(uncertainties))
    
    def retrain(self, training_data: pd.DataFrame, labels: pd.Series = None) -> Dict[str, Any]:
        """Risk mod√ºl√ºn√º yeniden eƒüit"""
        try:
            logger.info("Updating risk models and stress scenarios...")
            
            # VaR model performance optimization
            if len(training_data) > 100:
                # Ger√ßek uygulamada model backtesting ve parameter optimization
                updated_scenarios = len(self.stress_scenarios)
                model_improvement = np.random.uniform(0.05, 0.15)  # Simulated improvement
            else:
                updated_scenarios = 0
                model_improvement = 0.0
            
            self.last_training_date = datetime.now().isoformat()
            
            return {
                "status": "success",
                "updated_scenarios": updated_scenarios,
                "model_improvement": model_improvement,
                "training_samples": len(training_data),
                "training_date": self.last_training_date,
                "message": "Risk models and stress scenarios updated"
            }
            
        except Exception as e:
            logger.error(f"Error retraining risk module: {str(e)}")
            return {"status": "error", "message": str(e)}

if __name__ == "__main__":
    print("‚ö†Ô∏è ULTRA RISK MODULE - ENHANCED")
    print("="*50)
    
    # Test data
    test_data = {
        "symbol": "GARAN",
        "close": 45.80,
        "volume": 1500000,
        "timestamp": "2025-09-19T10:00:00"
    }
    
    # Module test
    risk_module = UltraRiskModule()
    
    print(f"‚úÖ Module initialized: {risk_module.name}")
    print(f"üìä Version: {risk_module.version}")
    print(f"üéØ Approach: VaR, Monte Carlo, Stress Testing")
    print(f"üîß Dependencies: {risk_module.dependencies}")
    
    # Test inference
    try:
        features = risk_module.prepare_features(test_data)
        result = risk_module.infer(features)
        
        print(f"\n‚ö†Ô∏è RISK ANALYSIS RESULT:")
        print(f"Score: {result.score:.2f}/100")
        print(f"Uncertainty: {result.uncertainty:.3f}")
        print(f"Confidence: {result.confidence_level}")
        print(f"Types: {result.type}")
        print(f"Explanation: {result.explanation}")
        
        # VaR model info
        var_models = getattr(risk_module, '_current_var_models', {})
        if var_models:
            print(f"\nüìä VaR Models (95% confidence):")
            for model_name, model_data in var_models.items():
                print(f"  - {model_name}: {model_data['var']:.3f} VaR, {model_data['expected_shortfall']:.3f} CVaR")
        
    except Exception as e:
        print(f"‚ùå Test error: {str(e)}")
    
    print(f"\nüöÄ Ultra Risk Module ready for Multi-Expert Engine!")